# -*- coding: utf-8 -*-
"""MARL BOIDS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Qb_8O4c3b9h1Ap1UeQ2ABjRwNwAfpuK
"""

!apt-get update
!apt-get install -y x11-utils python3-opengl xvfb
!pip install pyvirtualdisplay
import pyvirtualdisplay
display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))
display.start()
from vmas.simulator.scenario import BaseScenario
!pip install vmas

class MyScenario(BaseScenario):

    def make_world(self, batch_dim, device, **kwargs):
        raise NotImplementedError()

    def reset_world_at(self, env_index):
        raise NotImplementedError()

    def observation(self, agent):
        raise NotImplementedError()

    def reward(self, agent):
        raise NotImplementedError()

import torch
from torch import Tensor
from vmas.simulator.core import (
    Agent,
    Box,
    Landmark,
    Sphere,
    World
    )
from vmas.simulator.utils import (
    ScenarioUtils,
    Color
)

def make_world(self, batch_dim: int, device: torch.device, **kwargs):
  self.plot_grid = True

  '''*********** GETTING INFO FROM KWARGS ***********'''

  ''' Number of agents '''
  self.n_friends = kwargs.pop("friends", 5)
  self.n_enemies = kwargs.pop("enemies", 5)
  self.n_agents = self.n_friends + self.n_enemies
  self.n_teams = kwargs.pop("teams", 2)

  ''' Number of non-agent entities '''
  self.n_obstacles = kwargs.pop("n_obstacles", 2)
  self.n_goals = kwargs.pop("n_obstacles", 3)

  ''' cordinates for entities spawning '''
  self.world_spawning_x = kwargs.pop("world_spawning_x", 1)
  self.world_spawning_y = kwargs.pop("world_spawning_y", 1)

  ''' REWARD INFO '''
  ''' Penalize collision with another agent or obstacle '''
  self.agent_collision_penalty = kwargs.pop("agent_collision_penalty", -1)
  ''' Turning shared reward on '''
  self.shared_rew = kwargs.pop("shared_rew", False)

  ''' Minimum distance between entities at spawning time '''
  self.min_distance_between_entities = (kwargs.pop("agent_radius", 0.1) * 2 + 0.05)
  ''' Minimum distance between entities for collision trigger '''
  self.min_collision_distance = (0.005)

  ''' Warn if not all kwargs have been consumed '''
  ScenarioUtils.check_kwargs_consumed(kwargs)


  '''*********** MAKING THE WORLD ***********'''

  world = World(
    batch_dim=1,  # Number of environments
    device=device,  # Use your hardware (GPU/CPU)
    substeps=5,  # Substeps for simulation accuracy
    collision_force=500,  # Collision force for agent interaction
    dt=0.1,  # Simulation timestep
    drag=0.1,  # Optional drag for agent movement
    linear_friction=0.05,  # Optional friction
    angular_friction=0.02,  # Optional angular friction
  )

  known_colors = [
        Color.GREEN, # Team 1
        Color.RED    # Team 2
  ]

  self.goals = []

  #######################
  # Add agents
  #######################
  teams = {}
  for team in range(self.n_teams):
    teams[team] = []
    for agent_num in range(int(self.n_agents//self.n_teams)):

      agent = BoidAgent(
        collide=True,
        color=known_colors[team],
        render_action=True,
        position=1,
        velocity=1,
        sensor_range=1.0,
        max_speed=1.0,
        max_force=0.1
      )

      agent.pos_rew = torch.zeros(
        batch_dim, device=device
      )  # Tensor that will hold the position reward fo the agent
      agent.agent_collision_rew = (
        agent.pos_rew.clone()
      )  # Tensor that will hold the collision reward fo the agent

      teams[team].append(agent)
      world.add_agent(agent)

  #######################
  # Add Goals
  #######################

  for goal in range(self.n_goals):
    goal = Landmark(
      name=f"goal_{i}",
      collide=False,
      color=color,
    )
    world.add_landmark(goal)
    # agent.goal = goal
    self.goals.append(goal)

  #######################
  # Add Goals
  #######################
  ''' Add later '''

  return world

# MyScenario.make_world = make_world

n_agents = 8
n_teams = 2
teams = {}

for team in range(n_teams):
    teams[team] = []
    for agent in range(int(n_agents/2)):
      teams[team].append(agent)


print(teams)

class BoidAgent:
  def __init__(self, collide, color, render_action, position, velocity, sensor_range, max_speed, max_force):
    super().__init__()

    self.collide = collide  # Whether the agent collides with others
    self.color = color  # Color of the agent for rendering
    self.render_action = render_action  # Whether to render actions
    self.position = position  # Initial position
    self.velocity = velocity  # Initial velocity
    self.sensor_range = sensor_range  # Observation range
    self.max_speed = max_speed  # Maximum speed of the agent
    self.max_force = max_force  # Maximum force that can be applied

    # VMAS-specific attributes
    self.pos_rew = None  # Placeholder for position reward
    self.agent_collision_rew = None  # Placeholder for collision reward

  def update(self, steering_force):
    """Update the boid's velocity and position based on the steering force."""
    self.velocity += steering_force
    speed = self.velocity.norm()
    if speed > self.max_speed:
        self.velocity = self.velocity / speed * self.max_speed
    self.position += self.velocity

  def apply_force(self, force):
    """Apply a force to the boid."""
    self.velocity += force

def observation(agent, world):
  """
  Collect the relevant observation data for the given agent from the world.
  Only consider other Boids within the agent's sensor range.
  """
  obs = {
    "pos": agent.position,
    "vel": agent.velocity,
  }

  # Example: Relative positions and velocities of nearby agents (simple flocking)
  for other_agent in world.agents:
    if other_agent != agent:
      # Calculate the distance between agents
      distance = torch.norm(agent.position - other_agent.position)
      if distance <= agent.sensor_range:  # Check if within the sensor range
        # Relative position and velocity of nearby Boid
        relative_pos = other_agent.position - agent.position
        relative_vel = other_agent.velocity - agent.velocity
        obs[f"relative_pos_{id(other_agent)}"] = relative_pos
        obs[f"relative_vel_{id(other_agent)}"] = relative_vel

  # Optionally add more observations (like obstacles, goal positions, etc.)
  return obs

# def observation(self, agent, world):
#     obs = []

#     for entity in world.entities:
#         if entity is not agent:  # Skip self
#             dx = entity.state.p_pos[0] - agent.state.p_pos[0]
#             dy = entity.state.p_pos[1] - agent.state.p_pos[1]
#             dist = np.sqrt(dx**2 + dy**2)

#             if dist <= agent.lidar_range:  # Only include objects within range
#                 vx, vy = entity.state.v if hasattr(entity.state, "v") else (0, 0)
#                 obj_type = entity.type  # Assuming entities have a 'type' attribute

#                 obs.append([dx, dy, vx, vy, obj_type])

#     return np.array(obs).flatten()  # Flatten so VMAS can handle it